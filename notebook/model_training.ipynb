{"cells":[{"cell_type":"code","execution_count":28,"id":"b240fb00","metadata":{},"outputs":[],"source":["argentina={   \n","    \"dataset\": {\n","        \"country\": \"argentina\",\n","        \"sales_org\": \"AR00\",\n","        \"grouping_cols\": [\n","            \"cust_id\", \n","            \"city\",\n","            \"customer_classification\",\n","            \"subsector\", \n","            \"category\",\n","            \"brand\",\n","            \"product_key\"\n","        ],\n","        \"product_features_cols\":[\n","            \"product_key\", \n","            \"subsector\", \n","            \"category\",\n","            \"brand\"\n","        ],\n","        \"customer_features_cols\":[\n","            \"cust_id\", \n","            \"city\",\n","            \"customer_classification\"\n","        ],\n","        \"columns\": [],\n","        \"column_to_predict\": \"target\",\n","        \"columns_with_attr\": [],\n","        \"test_size\": 0.2,\n","        \"nfolds\" : 3,\n","        \"num_samples\": 5,\n","        \"num_threads\": 1,\n","        \"recency\": 180\n","    },\n","    \"models\": {\n","\n","    }\n","}"]},{"cell_type":"code","execution_count":29,"id":"f5cc37d5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<gcsfs.core.GCSFileSystem object at 0x7f645948e9a0>\n"]}],"source":["import pickle\n","import gcsfs\n","\n","\"\"\"Importing dict_positive from gcs\"\"\"\n","interim_fs = gcsfs.GCSFileSystem(project= \"tiger-mle\")\n","pickle_file = f\"gs://pg-explore/data/magento/interim/dict_positive.pkl\"\n","with interim_fs.open(pickle_file, 'rb') as handle:\n","        pickle_obj = pickle.load(handle)\n","dict_positive = pickle_obj['Argentina']   \n","\n","\"\"\"Importing dict_dataset from gcs\"\"\"\n","pickle_file = f\"gs://pg-explore/data/magento/interim/dict_dataset.pkl\"\n","with interim_fs.open(pickle_file, 'rb') as handle:\n","        pickle_obj = pickle.load(handle)\n","dict_dataset = pickle_obj['Argentina']   \n","\n","\"\"\"Importing dict_user_feature_matrix from gcs\"\"\"\n","pickle_file = f\"gs://pg-explore/data/magento/interim/dict_user_feature_matrix.pkl\"\n","print(interim_fs)\n","with interim_fs.open(pickle_file, 'rb') as handle:\n","        pickle_obj = pickle.load(handle)\n","dict_user_feature_matrix = pickle_obj['Argentina']   \n","\n","\"\"\"Importing dict_item_feature_matrix from gcs\"\"\"\n","pickle_file = f\"gs://pg-explore/data/magento/interim/dict_item_feature_matrix.pkl\"\n","with interim_fs.open(pickle_file, 'rb') as handle:\n","        pickle_obj = pickle.load(handle)\n","dict_item_feature_matrix = pickle_obj['Argentina']   "]},{"cell_type":"code","execution_count":null,"id":"5c762931","metadata":{},"outputs":[],"source":["\"\"\"Model training.\"\"\"\n","import inspect\n","from collections import namedtuple\n","\n","\n","class ModelTraining(PythonStep):\n","    \"\"\"Example Model training step.\"\"\"\n","\n","    output_binding = namedtuple(\"Results\", [\"dict_best_model\"])\n","\n","    def run(\n","        self,\n","        dict_positive,\n","        dict_dataset,\n","        dict_user_feature_matrix,\n","        dict_item_feature_matrix,\n","        model_name,\n","        build_id,\n","        countries,\n","        run\n","    ):\n","        \"\"\"Run model training step.\"\"\"\n","        params_list = inspect.signature(ModelTraining.run).parameters\n","        casted = get_data_class(locals(), params_list, ModelTrainingDataClass)\n","\n","        dict_test_score = {}\n","        dict_best_model = {}\n","\n","        for country in casted.countries:\n","            params = Parameters(country)\n","\n","            print(\"cross validation country:\", country)\n","\n","            df_positive = dict_positive[country]\n","            df_dataset = dict_dataset[country]\n","            user_feature_matrix = dict_user_feature_matrix[country]\n","            item_feature_matrix = dict_item_feature_matrix[country]\n","\n","            test_score, best_model = perform_cross_validation(\n","                df_positive, df_dataset, user_feature_matrix, item_feature_matrix, params)\n","\n","            run.log(\"auc\", test_score)\n","            run.log(\"hyperparameters\", best_model.get_params())\n","\n","            dict_test_score[country] = test_score\n","            dict_best_model[country] = best_model\n","\n","            register_country_model(\n","                run, country, best_model, casted.model_name, casted.build_id)\n","\n","\n","        return self.output_binding(dict_best_model)\n","\n","\n","if __name__ == \"__main__\":\n","    ModelTraining().azureml_process()\n"]},{"cell_type":"code","execution_count":30,"id":"67cdeb6b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cross validation country: Argentina\n","Random search started.\n","06:24:31, evaluating the hyperparameter set NO.1...\n","  06:24:32, Training and evaluating with fold NO.1...\n","  06:24:52, Training and evaluating with fold NO.2...\n","  06:25:11, Training and evaluating with fold NO.3...\n","Validation score for hyperparams set {'no_components': 58, 'learning_schedule': 'adagrad', 'learning_rate': 0.03192050446910579, 'item_alpha': 1.6999157579443403e-08, 'user_alpha': 3.7893668634965724e-08, 'max_sampled': 6, 'num_epochs': 21} is 0.5005416870117188.\n","\n","06:25:30, evaluating the hyperparameter set NO.2...\n","  06:25:31, Training and evaluating with fold NO.1...\n","  06:25:45, Training and evaluating with fold NO.2...\n","  06:25:59, Training and evaluating with fold NO.3...\n","Validation score for hyperparams set {'no_components': 59, 'learning_schedule': 'adadelta', 'learning_rate': 0.00023352205801679228, 'item_alpha': 1.406091422778282e-10, 'user_alpha': 9.078365144124506e-09, 'max_sampled': 13, 'num_epochs': 9} is 0.4985194504261017.\n","\n","06:26:12, evaluating the hyperparameter set NO.3...\n","  06:26:13, Training and evaluating with fold NO.1...\n","  06:26:54, Training and evaluating with fold NO.2...\n","  06:27:36, Training and evaluating with fold NO.3...\n","Validation score for hyperparams set {'no_components': 41, 'learning_schedule': 'adadelta', 'learning_rate': 0.0026289826019374094, 'item_alpha': 3.4509231769916847e-09, 'user_alpha': 2.862529287246853e-08, 'max_sampled': 7, 'num_epochs': 40} is 0.49462413787841797.\n","\n","06:28:17, evaluating the hyperparameter set NO.4...\n","  06:28:18, Training and evaluating with fold NO.1...\n","  06:28:23, Training and evaluating with fold NO.2...\n","  06:28:28, Training and evaluating with fold NO.3...\n","Validation score for hyperparams set {'no_components': 29, 'learning_schedule': 'adagrad', 'learning_rate': 0.07616005815988332, 'item_alpha': 1.056841812893885e-09, 'user_alpha': 2.9528249984143474e-09, 'max_sampled': 8, 'num_epochs': 10} is 0.49838411808013916.\n","\n","06:28:33, evaluating the hyperparameter set NO.5...\n","  06:28:34, Training and evaluating with fold NO.1...\n","  06:28:54, Training and evaluating with fold NO.2...\n","  06:29:14, Training and evaluating with fold NO.3...\n","Validation score for hyperparams set {'no_components': 28, 'learning_schedule': 'adagrad', 'learning_rate': 0.14896520090065712, 'item_alpha': 9.009292265229976e-09, 'user_alpha': 6.557549635222616e-09, 'max_sampled': 5, 'num_epochs': 43} is 0.5006375908851624.\n","\n","Evaluating the model with the best hyperparameters...\n","Test score is 0.4997359812259674.\n","Fitting the whole dataset with the best hyperparameter set...\n","Done.\n","auc 0.49973598\n","hyperparameters {'loss': 'warp', 'learning_schedule': 'adagrad', 'no_components': 28, 'learning_rate': 0.14896520090065712, 'k': 5, 'n': 10, 'rho': 0.95, 'epsilon': 1e-06, 'max_sampled': 5, 'item_alpha': 9.009292265229976e-09, 'user_alpha': 6.557549635222616e-09, 'random_state': RandomState(MT19937) at 0x7F6459088B40}\n","{'Argentina': <lightfm.lightfm.LightFM object at 0x7f6449e879a0>}\n"]}],"source":["from datetime import datetime\n","from itertools import islice\n","from sys import stdout\n","\n","from lightfm import LightFM\n","from lightfm.evaluation import auc_score\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","\n","def fit_model(\n","    interactions,\n","    *,\n","    weights=None,\n","    user_features=None,\n","    item_features=None,\n","    num_epochs=1,\n","    num_threads=1,\n","    loss=\"warp\",\n","    **kwargs,\n","):\n","    \"\"\"See https://making.lyst.com/lightfm/docs/lightfm.html#lightfm.LightFM.fit.\"\"\"\n","    hyperparams = {**kwargs}\n","    hyperparams[\"loss\"] = loss\n","    model = LightFM(**hyperparams)\n","    model.fit(\n","        interactions,\n","        sample_weight=weights,\n","        user_features=user_features,\n","        item_features=item_features,\n","        epochs=num_epochs,\n","        num_threads=num_threads,\n","    )\n","    return model\n","\n","class RandomSearchCV:\n","    \"\"\"\n","    Randomized search on hyper parameters.\n","\n","    Parameters\n","    ----------\n","    df_positive: pandas.DataFrame\n","        Dataframe contains unique positive interactions of user-item pairs,\n","        has columns of the form ['user', 'item'] or ['user', 'item', 'weights']\n","    dataset: lightfm.data.Dataset\n","        Lightfm dataset that has been fitted with the user/item ids and feature names.\n","    user_features: csr_matrix of shape [n_users, n_users + n_user_features], optional\n","        Each row contains that user’s weights over features.\n","    item_features: csr_matrix of shape [n_items, n_items + n_item_features], optional\n","        Each row contains that item’s weights over features.\n","    test_size: float\n","        Proportion of the dataset to include in the test split.\n","    nfolds: int, optional\n","        Number of folds in k-fold cv, must be at least 2. Defaults to 5.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        df_positive,\n","        dataset,\n","        user_features=None,\n","        item_features=None,\n","        test_size=0.1,\n","        nfolds=5,\n","    ):\n","        self.df_positive = df_positive\n","        self.dataset = dataset\n","        self.user_features = user_features\n","        self.item_features = item_features\n","        self.train, self.test = train_test_split(df_positive, test_size=test_size)\n","        self.nfolds = nfolds\n","\n","    def sample_hyperparameters(self):\n","        \"\"\"Code from https://stackoverflow.com/a/49983651.\"\"\"\n","        while True:\n","            yield {\n","                \"no_components\": np.random.randint(16, 64),\n","                \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n","                \"learning_rate\": np.random.exponential(0.05),\n","                \"item_alpha\": np.random.exponential(1e-8),\n","                \"user_alpha\": np.random.exponential(1e-8),\n","                \"max_sampled\": np.random.randint(5, 15),\n","                \"num_epochs\": np.random.randint(5, 50),\n","            }\n","\n","    def train_test_split_(self):\n","        \"\"\"Build interaction and weight matrices for train and test sets.\"\"\"\n","        train_interactions, train_weights = self.dataset.build_interactions(\n","            self.train.to_numpy()\n","        )\n","        test_interactions, _ = self.dataset.build_interactions(self.test.to_numpy())\n","        return train_interactions, train_weights, test_interactions\n","\n","    def kfold_split(self):\n","        \"\"\"Yield interaction and weight matrices from k-fold train and validation sets.\"\"\"\n","        kf = KFold(n_splits=self.nfolds, shuffle=True)\n","        for train_index, val_index in kf.split(self.train):\n","            # Train_interactions will be a matrix with SKU and Store with a marker without\n","            # indicating the strength of the relationship (TBC),\n","            # train_weights actually strength of the relationship.\n","            train_interactions, train_weights = self.dataset.build_interactions(\n","                self.train.iloc[train_index, :].to_numpy()\n","            )\n","            interactions_val, _ = self.dataset.build_interactions(\n","                self.train.iloc[val_index, :].to_numpy()\n","            )\n","            yield train_interactions, train_weights, interactions_val\n","\n","    def random_search(self, num_samples=10, num_threads=1, loss=\"warp\"):\n","        \"\"\"Random search hyperparameters using k-fold cross-validation.\"\"\"\n","        for i, hyperparams in enumerate(\n","            islice(self.sample_hyperparameters(), num_samples), 1\n","        ):\n","            now = datetime.now().strftime(\"%H:%M:%S\")\n","            stdout.write(f\"{now}, evaluating the hyperparameter set NO.{i}...\\n\")\n","            scores = []\n","            for no_fold, (\n","                train_interactions,\n","                train_weights,\n","                val_interactions,\n","            ) in enumerate(self.kfold_split(), 1):\n","                now = datetime.now().strftime(\"%H:%M:%S\")\n","                stdout.write(\n","                    f\"  {now}, Training and evaluating with fold NO.{no_fold}...\\n\"\n","                )\n","                model = fit_model(\n","                    interactions=train_interactions,\n","                    weights=train_weights,\n","                    user_features=self.user_features,\n","                    item_features=self.item_features,\n","                    num_threads=num_threads,\n","                    loss=loss,\n","                    **hyperparams,\n","                )\n","                validation_score = auc_score(\n","                    model,\n","                    train_interactions=train_interactions,\n","                    test_interactions=val_interactions,\n","                    user_features=self.user_features,\n","                    item_features=self.item_features,\n","                ).mean()\n","                scores.append(validation_score)\n","            score = np.mean(scores)\n","            stdout.write(\n","                f\"Validation score for hyperparams set {hyperparams} is {score}.\\n\\n\"\n","            )\n","            yield (score, hyperparams)\n","\n","    def select_hyperparams(self, num_samples=10, num_threads=1, loss=\"warp\"):\n","        \"\"\"Select hyperparameters using k-fold cross-validation.\"\"\"\n","        # Select the parameters associated with the maximum AUC from random_search,\n","        # lambda x:x[0] specifies AUC\n","        stdout.write(\"Random search started.\\n\")\n","        _, hyperparams = max(\n","            self.random_search(\n","                num_samples=num_samples,\n","                num_threads=num_threads,\n","                loss=loss,\n","            ),\n","            key=lambda x: x[0],\n","        )\n","        # We are getting the AUC from entire dataset:\n","        # (train - 90% of entire dataset, test- 10% of entire dataset)\n","        stdout.write(\"Evaluating the model with the best hyperparameters...\\n\")\n","        train_interactions, train_weights, test_interactions = self.train_test_split_()\n","        model = fit_model(\n","            interactions=train_interactions,\n","            weights=train_weights,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","            num_threads=num_threads,\n","            loss=loss,\n","            **hyperparams,\n","        )\n","        test_score = auc_score(\n","            model,\n","            train_interactions=train_interactions,\n","            test_interactions=test_interactions,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","        ).mean()\n","        stdout.write(f\"Test score is {test_score}.\\n\")\n","        return test_score, hyperparams\n","\n","    def select_model(\n","        self, num_samples: int = 10, num_threads: int = 1, loss: str = \"warp\"\n","    ):\n","        \"\"\"\n","        Train the model with the whole dataset and the best hyperparameters.\n","\n","        Parameters\n","        ----------\n","        num_samples\n","            Number of combinations of hyperparameters to try.\n","        num_epochs\n","            Number of epochs to run.\n","        num_threads\n","            Number of parallel computation threads to use.\n","            Should not be higher than the number of physical cores.\n","        loss\n","            One of (‘logistic’, ‘bpr’, ‘warp’, ‘warp-kos’): the loss function.\n","\n","            We suggest to use warp-kos if your rating dataframe doesn't have a\n","            third column, warp if it does. They perform the best and they are the\n","            distinguishing features of the lightfm package.\n","\n","            See https://making.lyst.com/lightfm/docs/lightfm.html#lightfm.LightFM\n","\n","        Returns\n","        -------\n","        test_score: float\n","            Test auc to report.\n","        model: LightFM instance\n","            The trained model.\n","        \"\"\"\n","        test_score, hyperparams = self.select_hyperparams(\n","            num_samples=num_samples,\n","            num_threads=num_threads,\n","            loss=loss,\n","        )\n","        stdout.write(\"Fitting the whole dataset with the best hyperparameter set...\\n\")\n","        train_interactions, train_weights = self.dataset.build_interactions(\n","            self.df_positive.to_numpy()\n","        )\n","        model = fit_model(\n","            interactions=train_interactions,\n","            weights=train_weights,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","            num_threads=num_threads,\n","            loss=loss,\n","            **hyperparams,\n","        )\n","        stdout.write(\"Done.\\n\")\n","        return test_score, model\n","\n","def perform_cross_validation(\n","    df_positive,\n","    df_dataset,\n","    user_feature_matrix,\n","    item_feature_matrix,\n","    params\n","):\n","    \"\"\"Feed the 4 outputs got from the model input stage to this stage.\n","\n","    Add loss=\"warp-kos\" to select_model.\n","    If your df_positive doesn't have a 3rd rating column.\n","    Note that because the demo data are generated randomly.\n","    Any good model will only produce around 50% AUC.\n","    \"\"\"\n","    test_score, best_model = RandomSearchCV(\n","        df_positive,\n","        df_dataset,\n","        user_features=user_feature_matrix,\n","        item_features=item_feature_matrix,\n","        test_size=params[\"dataset\"][\"test_size\"],\n","        nfolds=params[\"dataset\"][\"nfolds\"]) \\\n","        .select_model(num_samples=params[\"dataset\"][\"num_samples\"],\n","                      num_threads=params[\"dataset\"][\"num_threads\"])\n","    return test_score, best_model\n","\n","\n","def run(\n","        dict_positive,\n","        dict_dataset,\n","        dict_user_feature_matrix,\n","        dict_item_feature_matrix\n","    ):\n","        \"\"\"Run model training step.\"\"\"\n","        \n","        dict_test_score = {}\n","        dict_best_model = {}\n","\n","        params = argentina\n","        country = 'Argentina'\n","        print(\"cross validation country:\", country)\n","        \n","        test_score, best_model = perform_cross_validation(\n","            dict_positive,dict_dataset,dict_user_feature_matrix,dict_item_feature_matrix, params)\n","\n","        print(\"auc\", test_score)\n","        print(\"hyperparameters\", best_model.get_params())\n","\n","        dict_test_score[country] = test_score\n","        dict_best_model[country] = best_model\n","\n","        return (dict_best_model)\n","\n","dict_best_model = run(dict_positive,dict_dataset,dict_user_feature_matrix,dict_item_feature_matrix)\n","print(dict_best_model)"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}